{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "  - Logistic Regression: Used for classification tasks (binary or multiclass), output is a probability between 0 and 1.\n",
        "  - Linear Regression: Used for regression tasks (predicting continuous values), output is a real number.\n",
        "\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "  - P(y=1‚à£x)=1/1+e ‚àí(Œ≤ 0‚Äã +Œ≤ 1x 1‚Äã +Œ≤ 2x 2‚Äã +...+Œ≤ nx n‚Äã)\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "   - It maps any real number to a value between 0 and 1.\n",
        "   - Useful for modeling probabilities of class membership.\n",
        "4. What is the cost function of Logistic Regression?\n",
        "  - Cost(h Œ∏(x),y)=[y‚ãÖlog(h Œ∏(x))+(1‚àíy)‚ãÖlog(1‚àíh Œ∏c(x))]\n",
        "5. What is Regularization in Logistic Regression?\n",
        "  - A technique to prevent overfitting.\n",
        "  - It adds a penalty to large coefficients in the cost function.\n",
        "6. Why is it needed Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "  - Needed to reduce overfitting and improve generalization.\n",
        "  - Lasso (L1): Shrinks some coefficients to zero (performs feature selection).\n",
        "  - Ridge (L2): Shrinks coefficients smoothly but does not set them to zero.\n",
        "  - Elastic Net: A mix of L1 and L2; combines benefits of both.\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "  - Use Elastic Net when:\n",
        "    - You have many correlated features.\n",
        "    - You want both feature selection and regularization.\n",
        "8. What is the impact of the regularization parameter (Œª) in Logistic 9. Regression?\n",
        "  - High Œª: Strong regularization, may underfit.\n",
        "  - Low Œª: Weak regularization, may overfit.\n",
        "  - It controls the trade-off between fitting the data and keeping coefficients small.\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "  - No multicollinearity among predictors.\n",
        "  - Linear relationship between independent variables and the log-odds.\n",
        "  - Large sample size.\n",
        "  - Independence of observations.\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "  - Decision Trees\n",
        "  - Random Forest\n",
        "  - Support Vector Machines (SVM)\n",
        "  - K-Nearest Neighbors (KNN)\n",
        "  - Naive Bayes\n",
        "  - Neural Networks\n",
        "11. What are Classification Evaluation Metrics ?\n",
        "  - Accuracy\n",
        "  - Precision\n",
        "  - Recall\n",
        "  - F1-Score\n",
        "  - ROC-AUC Score\n",
        "  - Confusion Matrix\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "  - Can lead to biased predictions toward the majority class.\n",
        "  - Solutions:\n",
        "    - Resampling techniques\n",
        "    - Class weights\n",
        "    - SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "  - Adjusting model parameters like:\n",
        "  - Regularization strength (Œª or C)\n",
        "  - Type of penalty (L1, L2)\n",
        "  - Solver\n",
        "  - Done using Grid Search or Random Search with Cross-validation.\n",
        "14. What are different solvers in Logistic Regression?\n",
        "  - liblinear: Good for small datasets, supports L1 and L2.\n",
        "  - saga: Supports Elastic Net and works with large datasets.\n",
        "  - lbfgs: Good for L2 penalty and multiclass classification.\n",
        "  - newton-cg: Like lbfgs but uses Newton‚Äôs method.\n",
        "15. Which one should be used How is Logistic Regression extended for multiclass\n",
        "classification?\n",
        "  - Use liblinear for small datasets.\n",
        "  - Use saga/lbfgs for large datasets or when using Elastic Net.\n",
        "  - For multiclass:\n",
        "    - One-vs-Rest (OvR): Builds a binary classifier for each class.\n",
        "    - Softmax (Multinomial): Generalization of logistic regression for multiple classes.\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "  - Advantages:\n",
        "    - Simple and interpretable\n",
        "    - Fast and efficient\n",
        "    - Works well with linearly separable data\n",
        "  - Disadvantages:\n",
        "    - Assumes linear relationship (log-odds)\n",
        "    - Not suitable for complex relationships\n",
        "    - Can underperform on high-dimensional data\n",
        "17. What are some use cases of Logistic Regression?\n",
        "  - Spam detection\n",
        "  - Credit scoring\n",
        "  - Disease diagnosis\n",
        "  - Customer churn prediction\n",
        "  - Fraud detection\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "  -  Logistic Regression: For binary classification.\n",
        "  - Softmax Regression: For multiclass classification, outputs probabilities for all classes.\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "  - OvR: Simple, effective for small datasets.\n",
        "  - Softmax: Better when classes are mutually exclusive and correlated.\n",
        "\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "  - Coefficients show the change in log-odds of the outcome for a one-unit change in the predictor. ùëíùõΩùëñeŒ≤i‚Äã\n",
        " : Tells how the odds change when ùë•ùëñx i‚Äã increases by 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mM3dHThuFIa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        " from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Create and train the model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and check accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V0HfXUuLoNS",
        "outputId": "6eaba541-e3ee-4e18-b174-976c953606ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with L1:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxNaCgdRL-c8",
        "outputId": "87f7c88d-f6bf-42bd-8bf4-41d7dcc72c85"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with L1: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YYPIU8JMAGB",
        "outputId": "21dd1c48-18f6-4566-a1da-c57c039766ec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Coefficients: [[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with Elastic Net:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5paDVQIhMBKG",
        "outputId": "405210f3-60d0-431a-bd33-87d8fe529947"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Elastic Net: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Multiclass Accuracy (OvR):\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNv7JH_zMCHO",
        "outputId": "64e9f746-7588-41e5-cef9-3ac34429c84c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Accuracy (OvR): 0.8666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "print(\"Accuracy with best parameters:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwkEhm0KMC1q",
        "outputId": "2c7f4d35-87a0-46de-e107-01b5b6202cf5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Accuracy with best parameters: 0.9333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.99166667]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pwvxQa9MEBp",
        "outputId": "da771e26-f398-4e74-ce63-c5af4c7d8d9f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bFcku3BYME7O",
        "outputId": "b0ef76ba-c93d-4cdc-fd80-be38ef22a417"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-db26772b8f53>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "print(\"Accuracy with best parameters:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwQOxlppMFrl",
        "outputId": "264c9296-a4e4-45ea-c40d-c09bff9e4d36"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': 1}\n",
            "Accuracy with best parameters: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "ovo_model = OneVsOneClassifier(model)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U9XugYAMGgw",
        "outputId": "d23134e0-b916-4a5e-aae4-6f35170da250"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "pp2cB57jMItY",
        "outputId": "6d1d8950-0b43-43bd-f8e5-5885c78a1672"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAHWCAYAAAD0P8cUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAO+BJREFUeJzt3Xl4Dff+B/D3JOQksguyIIklDYnYVYlartSaENoSXIJSVUuJNa0QaxQlRUq1vaSK2sOltgpSFWuEVFuEoLfELqlITjSZ3x+enF+PJOTInEzOzPvVZ55HZuZ85zPnSX18PvOdGUEURRFEREQKYiZ3AERERFJjciMiIsVhciMiIsVhciMiIsVhciMiIsVhciMiIsVhciMiIsVhciMiIsVhciMiIsVhciOTcvnyZXTq1An29vYQBAFxcXGSjn/t2jUIgoA1a9ZIOq4pa9++Pdq3by93GEQGYXIjg125cgUjRoxA7dq1YWlpCTs7O/j7++Pzzz9Hdna2UY8dGhqKlJQUzJ07F2vXrkXz5s2NeryyNHjwYAiCADs7uyK/x8uXL0MQBAiCgEWLFhk8/s2bNxEZGYnk5GQJoiUq3yrIHQCZlt27d+Pdd9+FRqPBoEGD0KBBA+Tm5uLo0aOYNGkSLly4gFWrVhnl2NnZ2UhMTMQnn3yC0aNHG+UYHh4eyM7ORsWKFY0y/stUqFABT548wX//+1/06dNHb9u6detgaWmJnJycVxr75s2bmDlzJjw9PdG4ceMSf27//v2vdDwiOTG5UYmlpaUhJCQEHh4eiI+Ph6urq27bqFGjkJqait27dxvt+Hfv3gUAODg4GO0YgiDA0tLSaOO/jEajgb+/PzZs2FAoua1fvx7du3fH1q1byySWJ0+eoFKlSrCwsCiT4xFJiW1JKrEFCxbg8ePH+Oabb/QSW4G6devio48+0v38999/Y/bs2ahTpw40Gg08PT3x8ccfQ6vV6n3O09MTgYGBOHr0KF5//XVYWlqidu3a+Pbbb3X7REZGwsPDAwAwadIkCIIAT09PAM/aeQV//qfIyEgIgqC37sCBA2jTpg0cHBxgY2MDb29vfPzxx7rtxV1zi4+Px5tvvglra2s4ODigZ8+e+O2334o8XmpqKgYPHgwHBwfY29tjyJAhePLkSfFf7HP69++PPXv24NGjR7p1p06dwuXLl9G/f/9C+z948AATJ06En58fbGxsYGdnh65du+LcuXO6fQ4fPowWLVoAAIYMGaJrbxacZ/v27dGgQQOcOXMGbdu2RaVKlXTfy/PX3EJDQ2FpaVno/Dt37gxHR0fcvHmzxOdKZCxMblRi//3vf1G7dm20bt26RPsPGzYM06dPR9OmTbFkyRK0a9cOUVFRCAkJKbRvamoq3nnnHbz11lv47LPP4OjoiMGDB+PChQsAgN69e2PJkiUAgH79+mHt2rWIjo42KP4LFy4gMDAQWq0Ws2bNwmeffYYePXrg559/fuHnfvzxR3Tu3Bl37txBZGQkwsLCcOzYMfj7++PatWuF9u/Tpw/++usvREVFoU+fPlizZg1mzpxZ4jh79+4NQRCwbds23br169ejXr16aNq0aaH9r169iri4OAQGBmLx4sWYNGkSUlJS0K5dO12iqV+/PmbNmgUAeP/997F27VqsXbsWbdu21Y1z//59dO3aFY0bN0Z0dDQ6dOhQZHyff/45qlatitDQUOTl5QEAvvzyS+zfvx/Lli2Dm5tbic+VyGhEohLIyMgQAYg9e/Ys0f7JyckiAHHYsGF66ydOnCgCEOPj43XrPDw8RABiQkKCbt2dO3dEjUYjTpgwQbcuLS1NBCAuXLhQb8zQ0FDRw8OjUAwzZswQ//krvmTJEhGAePfu3WLjLjjG6tWrdesaN24sVqtWTbx//75u3blz50QzMzNx0KBBhY43dOhQvTF79eolOjk5FXvMf56HtbW1KIqi+M4774gdO3YURVEU8/LyRBcXF3HmzJlFfgc5OTliXl5eofPQaDTirFmzdOtOnTpV6NwKtGvXTgQgrly5ssht7dq101u3b98+EYA4Z84c8erVq6KNjY0YHBz80nMkKius3KhEMjMzAQC2trYl2v+HH34AAISFhemtnzBhAgAUujbn4+ODN998U/dz1apV4e3tjatXr75yzM8ruFa3Y8cO5Ofnl+gzt27dQnJyMgYPHozKlSvr1jds2BBvvfWW7jz/6YMPPtD7+c0338T9+/d132FJ9O/fH4cPH0Z6ejri4+ORnp5eZEsSeHadzszs2f/KeXl5uH//vq7lmpSUVOJjajQaDBkypET7durUCSNGjMCsWbPQu3dvWFpa4ssvvyzxsYiMjcmNSsTOzg4A8Ndff5Vo/+vXr8PMzAx169bVW+/i4gIHBwdcv35db727u3uhMRwdHfHw4cNXjLiwvn37wt/fH8OGDYOzszNCQkKwadOmFya6gji9vb0Lbatfvz7u3buHrKwsvfXPn4ujoyMAGHQu3bp1g62tLTZu3Ih169ahRYsWhb7LAvn5+ViyZAm8vLyg0WhQpUoVVK1aFefPn0dGRkaJj1m9enWDJo8sWrQIlStXRnJyMpYuXYpq1aqV+LNExsbkRiViZ2cHNzc3/PLLLwZ97vkJHcUxNzcvcr0oiq98jILrQQWsrKyQkJCAH3/8EQMHDsT58+fRt29fvPXWW4X2LY3SnEsBjUaD3r17IzY2Ftu3by+2agOAefPmISwsDG3btsV3332Hffv24cCBA/D19S1xhQo8+34McfbsWdy5cwcAkJKSYtBniYyNyY1KLDAwEFeuXEFiYuJL9/Xw8EB+fj4uX76st/727dt49OiRbuajFBwdHfVmFhZ4vjoEADMzM3Ts2BGLFy/Gr7/+irlz5yI+Ph6HDh0qcuyCOC9evFho2++//44qVarA2tq6dCdQjP79++Ps2bP466+/ipyEU2DLli3o0KEDvvnmG4SEhKBTp04ICAgo9J2U9B8aJZGVlYUhQ4bAx8cH77//PhYsWIBTp05JNj5RaTG5UYlNnjwZ1tbWGDZsGG7fvl1o+5UrV/D5558DeNZWA1BoRuPixYsBAN27d5csrjp16iAjIwPnz5/Xrbt16xa2b9+ut9+DBw8KfbbgZubnb08o4OrqisaNGyM2NlYvWfzyyy/Yv3+/7jyNoUOHDpg9ezaWL18OFxeXYvczNzcvVBVu3rwZf/75p966giRc1D8EDDVlyhTcuHEDsbGxWLx4MTw9PREaGlrs90hU1ngTN5VYnTp1sH79evTt2xf169fXe0LJsWPHsHnzZgwePBgA0KhRI4SGhmLVqlV49OgR2rVrh5MnTyI2NhbBwcHFTjN/FSEhIZgyZQp69eqFsWPH4smTJ1ixYgVee+01vQkVs2bNQkJCArp37w4PDw/cuXMHX3zxBWrUqIE2bdoUO/7ChQvRtWtXtGrVCu+99x6ys7OxbNky2NvbIzIyUrLzeJ6ZmRmmTZv20v0CAwMxa9YsDBkyBK1bt0ZKSgrWrVuH2rVr6+1Xp04dODg4YOXKlbC1tYW1tTVatmyJWrVqGRRXfHw8vvjiC8yYMUN3a8Lq1avRvn17REREYMGCBQaNR2QUMs/WJBN06dIlcfjw4aKnp6doYWEh2traiv7+/uKyZcvEnJwc3X5Pnz4VZ86cKdaqVUusWLGiWLNmTTE8PFxvH1F8ditA9+7dCx3n+Snoxd0KIIqiuH//frFBgwaihYWF6O3tLX733XeFbgU4ePCg2LNnT9HNzU20sLAQ3dzcxH79+omXLl0qdIznp8v/+OOPor+/v2hlZSXa2dmJQUFB4q+//qq3T8Hxnr/VYPXq1SIAMS0trdjvVBT1bwUoTnG3AkyYMEF0dXUVraysRH9/fzExMbHIKfw7duwQfXx8xAoVKuidZ7t27URfX98ij/nPcTIzM0UPDw+xadOm4tOnT/X2Gz9+vGhmZiYmJia+8ByIyoIgigZc5SYiIjIBvOZGRESKw+RGRESKw+RGRESKw+RGRESKw+RGRESKw+RGRESKw+RGRESKo8gnlGw5d0vuEEglAn0Lv5GcyBgsJf7b2qrJaMnGyj67XLKxpKLI5EZERC8hKLtxp+yzIyIiVWLlRkSkRhK+Aqk8YnIjIlIjtiWJiIhMCys3IiI1YluSiIgUh21JIiIi08LKjYhIjdiWJCIixWFbkoiIyLSwciMiUiO2JYmISHHYliQiIjItrNyIiNSIbUkiIlIctiWJiIhMCys3IiI1YluSiIgUh21JIiIi08LkRkSkRoKZdIsBEhISEBQUBDc3NwiCgLi4uGL3/eCDDyAIAqKjow0+PSY3IiI1MhOkWwyQlZWFRo0aISYm5oX7bd++HcePH4ebm9srnR6vuRERUZnp2rUrunbt+sJ9/vzzT4wZMwb79u1D9+7dX+k4TG5ERGok4YQSrVYLrVart06j0UCj0Rg8Vn5+PgYOHIhJkybB19f3lWNiW5KISI0EQbIlKioK9vb2ektUVNQrhfXpp5+iQoUKGDt2bKlOj5UbERGVSnh4OMLCwvTWvUrVdubMGXz++edISkqCUMr78Fi5ERGpkYSzJTUaDezs7PSWV0luP/30E+7cuQN3d3dUqFABFSpUwPXr1zFhwgR4enoaNBYrNyIiNSqHTygZOHAgAgIC9NZ17twZAwcOxJAhQwwai8mNiIjKzOPHj5Gamqr7OS0tDcnJyahcuTLc3d3h5OSkt3/FihXh4uICb29vg47D5EZEpEYyPX7r9OnT6NChg+7ngmt1oaGhWLNmjWTHYXIjIlIjmdqS7du3hyiKJd7/2rVrr3QcTighIiLFYeVGRKRGCn8rAJMbEZEalcPZklJSduomIiJVYuVGRKRGbEsSEZHisC1JRERkWli5ERGpEduSRESkOApPbso+OyIiUiVWbkREaqTwCSVMbkREasS2JBERkWlh5UZEpEZsSxIRkeKwLUlERGRaWLkREakR25JERKQ0gsKTG9uSRESkOKzciIhUSOmVG5MbEZEaKTu3sS1JRETKw8qNiEiF2JYkIiLFUXpyY1uSiIgUh5UbEZEKKb1yY3IjIlIhpSc3tiWJiEhxWLkREamRsgs3JjciIjViW5KIiMjEsHIjIlIhpVduTG5ERCqk9OTGtiQRESkOKzciIhVSeuXG5EZEpEbKzm1sSxIRkfKwciMiUiG2JYmISHGUntzYliQiIsVh5UZEpEKs3IwoNzcXmzZtwvjx49GvXz/069cP48ePx+bNm5GbmytnaEREyiZIuBggISEBQUFBcHNzgyAIiIuL0217+vQppkyZAj8/P1hbW8PNzQ2DBg3CzZs3DT492ZJbamoq6tevj9DQUJw9exb5+fnIz8/H2bNnMWjQIPj6+iI1NVWu8IiIyAiysrLQqFEjxMTEFNr25MkTJCUlISIiAklJSdi2bRsuXryIHj16GHwcQRRFUYqADfXWW2/B2toa3377Lezs7PS2ZWZmYtCgQcjOzsa+ffsMHnvLuVtShUn0QoG+rnKHQCphKfFFJOdhmyUb6/bX777S5wRBwPbt2xEcHFzsPqdOncLrr7+O69evw93dvcRjy3bN7eeff8bJkycLJTYAsLOzw+zZs9GyZUsZIiMiUj4pr7lptVpotVq9dRqNBhqNptRjZ2RkQBAEODg4GPQ52dqSDg4OuHbtWrHbr127ZvDJEBFR2YuKioK9vb3eEhUVVepxc3JyMGXKFPTr16/IQuhFZKvchg0bhkGDBiEiIgIdO3aEs7MzAOD27ds4ePAg5syZgzFjxsgVHhGRoklZuYWHhyMsLExvXWmrtqdPn6JPnz4QRRErVqww+POyJbdZs2bB2toaCxcuxIQJE3RftCiKcHFxwZQpUzB58mS5wiMiUjQpk5tULcgCBYnt+vXriI+PN7hqA2S+z23KlCmYMmUK0tLSkJ6eDgBwcXFBrVq15AyLiIhkUpDYLl++jEOHDsHJyemVxikXN3HXqlWLCY2IqCzJdA/348eP9W7zSktLQ3JyMipXrgxXV1e88847SEpKwq5du5CXl6crfCpXrgwLC4sSH6dcJDciIipbcj2h5PTp0+jQoYPu54JrdaGhoYiMjMTOnTsBAI0bN9b73KFDh9C+ffsSH4fJjYiIykz79u3xoturpbr1msmNiEiFlP5sSSY3IiIVUnpyk/2VN3v37sXRo0d1P8fExKBx48bo378/Hj58KGNkRERkqmRPbpMmTUJmZiYAICUlBRMmTEC3bt2QlpZW6KZAIiKSiExvBSgrsrcl09LS4OPjAwDYunUrAgMDMW/ePCQlJaFbt24yR0dEpExsSxqZhYUFnjx5AgD48ccf0alTJwDP7mkoqOiIiIgMIXvl1qZNG4SFhcHf3x8nT57Exo0bAQCXLl1CjRo1ZI6OiEiZlF65yZ7cli9fjg8//BBbtmzBihUrUL16dQDAnj170KVLF5mjU7aDm1Yjfkus3roqbjUxPnqtTBGR0n2/fh1iV3+De/fu4jXvepj6cQT8GjaUOyxVYnIzMnd3d+zatavQ+iVLlsgQjfpUq+mJoRGf6X42MzOXMRpSsr17fsCiBVGYNmMm/PwaYd3aWIwc8R527Nr7ys8PJCqO7NfckpKSkJKSovt5x44dCA4Oxscff4zc3FwZI1MHMzNz2Do46RZrOwe5QyKFWhu7Gr3f6YPgXm+jTt26mDZjJiwtLRG3bavcoamSIAiSLeWR7MltxIgRuHTpEgDg6tWrCAkJQaVKlbB582a+8qYM3E//E/NHvI1Fo/th09I5eHTvttwhkQI9zc3Fb79ewButWuvWmZmZ4Y03WuP8ubMyRqZiCr8VQPbkdunSJd0DMjdv3oy2bdti/fr1WLNmDbZuffm/6LRaLTIzM/WWp7nal36OgBpePnj7w6kY/PEC9Bw2Hg/v3MJX08dCm/1E7tBIYR4+eoi8vLxC7UcnJyfcu3dPpqhIyWRPbqIoIj8/H8CzWwEK7m2rWbNmiX7pi3q9+fZvlhk1ZqXwbtISfq3aw8WjDrwav45B4fORnfUYKYmH5A6NiIxM6W1J2SeUNG/eHHPmzEFAQACOHDmie514WloanJ2dX/r5ol5vvvviA6PEqnRW1rao4lYD99P/lDsUUhhHB0eYm5vj/v37euvv37+PKlWqyBSVupXXpCQV2Su36OhoJCUlYfTo0fjkk09Qt25dAMCWLVvQunXrl3z62evN7ezs9JaKFtK97lxNtDlP8CD9JmwdOHONpFXRwgL1fXxx4niibl1+fj5OnEhEw0ZNZIyMlEr2yq1hw4Z6syULLFy4EObmnJZuTHu+/QL1mreGQxVnZD68j4ObVkMwM0OjNh3lDo0UaGDoEER8PAW+vg3QwK8hvlsbi+zsbAT36i13aKqk8MJN/uRWHEtLS7lDULyMB3ex8fPZePJXJqzt7OFRzw8fzP2CtwOQUXTp2g0PHzzAF8uX4t69u/CuVx9ffPk1nNiWlIXS25KCKNVrT19RXl4elixZgk2bNuHGjRuF7m178MDw62dbzt2SKjyiFwr0dZU7BFIJS4lLEa9JeyUb6/LC8vc0Kdmvuc2cOROLFy9G3759kZGRgbCwMPTu3RtmZmaIjIyUOzwiIkUSBOmW8kj25LZu3Tp89dVXmDBhAipUqIB+/frh66+/xvTp03H8+HG5wyMiUiSl3woge3JLT0+Hn58fAMDGxgYZGRkAgMDAQOzevVvO0IiIyETJntxq1KiBW7eeXSOrU6cO9u/fDwA4deoUNBpO6SciMga2JY2sV69eOHjwIABgzJgxiIiIgJeXFwYNGoShQ4fKHB0RkTKZmQmSLeWR7LcCzJ8/X/fnvn37wt3dHYmJifDy8kJQUJCMkRERkamSPbk9r1WrVmjVqpXcYRARKVp5bSdKRZbktnPnzhLv26NHDyNGQkRESiRLcgsODi7RfoIgIC8vz7jBEBGpUHmdwi8VWZJbwStuiIhIHgrPbfLPliQiIpKabMktPj4ePj4+yMzMLLQtIyMDvr6+SEhIkCEyIiLl4xNKjCQ6OhrDhw+HnZ1doW329vYYMWIElixZIkNkRETKx+RmJOfOnUOXLsU/SbpTp044c+ZMGUZERERKIdt9brdv30bFihWL3V6hQgXcvXu3DCMiIlKPclpwSUa2yq169er45Zdfit1+/vx5uLryXVlERMbAtqSRdOvWDREREcjJySm0LTs7GzNmzEBgYKAMkRERkamTrS05bdo0bNu2Da+99hpGjx4Nb29vAMDvv/+OmJgY5OXl4ZNPPpErPCIiRSunBZdkZEtuzs7OOHbsGEaOHInw8HCIogjgWancuXNnxMTEwNnZWa7wiIgUrby2E6Ui64OTPTw88MMPP+Dhw4dITU2FKIrw8vKCo6OjnGEREZGJKxdvBXB0dESLFi3kDoOISDUUXriVj+RGRERlS+ltST5bkoiIykxCQgKCgoLg5uYGQRAQFxent10URUyfPh2urq6wsrJCQEAALl++bPBxmNyIiFRIEKRbDJGVlYVGjRohJiamyO0LFizA0qVLsXLlSpw4cQLW1tbo3LlzkbeNvQjbkkREKiRXW7Jr167o2rVrkdtEUUR0dDSmTZuGnj17AgC+/fZbODs7Iy4uDiEhISU+Dis3IiIqFa1Wi8zMTL1Fq9UaPE5aWhrS09MREBCgW2dvb4+WLVsiMTHRoLGY3IiIVEjKtmRUVBTs7e31lqioKINjSk9PB4BC9zg7OzvrtpUU25JERCokZVsyPDwcYWFheus0Go1k478KJjciIioVjUYjSTJzcXEB8OytMf98cP7t27fRuHFjg8ZiW5KISIXkmi35IrVq1YKLiwsOHjyoW5eZmYkTJ06gVatWBo3Fyo2ISIXkmi35+PFjpKam6n5OS0tDcnIyKleuDHd3d4wbNw5z5syBl5cXatWqhYiICLi5uSE4ONig4zC5ERFRmTl9+jQ6dOig+7ngWl1oaCjWrFmDyZMnIysrC++//z4ePXqENm3aYO/evbC0tDToOIJY8Dh+Bdly7pbcIZBKBPryhbpUNiwlLkXaLPpJsrGOTnxTsrGkwsqNiEiF+GxJIiIiE8PKjYhIhZReuTG5ERGpkMJzG9uSRESkPKzciIhUiG1JIiJSHIXnNrYliYhIeVi5ERGpENuSRESkOArPbWxLEhGR8rByIyJSITOFl25MbkREKqTw3Ma2JBERKQ8rNyIiFeJsSSIiUhwzZec2tiWJiEh5WLkREakQ25JERKQ4Cs9tbEsSEZHysHIjIlIhAcou3ZjciIhUiLMliYiITAwrNyIiFeJsSSIiUhyF5za2JYmISHlYuRERqRBfeUNERIqj8NzGtiQRESkPKzciIhXibEkiIlIchec2tiWJiEh5WLkREakQZ0sSEZHiKDu1sS1JREQKxMqNiEiFOFuSiIgUh6+8ISIiMjGs3IiIVIhtSQA7d+4s8YA9evR45WCIiKhsKDy3lSy5BQcHl2gwQRCQl5dXmniIiEjB8vLyEBkZie+++w7p6elwc3PD4MGDMW3aNEmryRIlt/z8fMkOSERE8pOrLfnpp59ixYoViI2Nha+vL06fPo0hQ4bA3t4eY8eOlew4vOZGRKRCcs2WPHbsGHr27Inu3bsDADw9PbFhwwacPHlS0uO8UnLLysrCkSNHcOPGDeTm5uptkzLzEhFR+afVaqHVavXWaTQaaDSaQvu2bt0aq1atwqVLl/Daa6/h3LlzOHr0KBYvXixpTAYnt7Nnz6Jbt2548uQJsrKyULlyZdy7dw+VKlVCtWrVmNyIiEyAlG3JqKgozJw5U2/djBkzEBkZWWjfqVOnIjMzE/Xq1YO5uTny8vIwd+5cDBgwQLJ4gFe4z238+PEICgrCw4cPYWVlhePHj+P69eto1qwZFi1aJGlwRERkHIKES3h4ODIyMvSW8PDwIo+7adMmrFu3DuvXr0dSUhJiY2OxaNEixMbGSnp+BlduycnJ+PLLL2FmZgZzc3NotVrUrl0bCxYsQGhoKHr37i1pgEREVL4V14IsyqRJkzB16lSEhIQAAPz8/HD9+nVERUUhNDRUspgMrtwqVqwIM7NnH6tWrRpu3LgBALC3t8cff/whWWBERGQ8ZoIg2WKIJ0+e6HJIAXNzc8ln5RtcuTVp0gSnTp2Cl5cX2rVrh+nTp+PevXtYu3YtGjRoIGlwRERkHHLdxB0UFIS5c+fC3d0dvr6+OHv2LBYvXoyhQ4dKehyDK7d58+bB1dUVADB37lw4Ojpi5MiRuHv3LlatWiVpcEREpCzLli3DO++8gw8//BD169fHxIkTMWLECMyePVvS4wiiKIqSjlgObDl3S+4QSCUCfV3lDoFUwlLiu5Lf33xBsrFWvesr2VhS4U3cREQqxGdLPqdWrVovvD/i6tWrpQqIiIiotAxObuPGjdP7+enTpzh79iz27t2LSZMmSRUXEREZkaGzHE2Nwcnto48+KnJ9TEwMTp8+XeqAiIjI+BSe26R7E3fXrl2xdetWqYYjIiJ6ZZJNKNmyZQsqV64s1XBERGREfBP3c5o0aaL3pYiiiPT0dNy9exdffPGFpMG9Kk7PprLi2GK03CGQSmSfXS7peJK17copg5Nbz5499ZKbmZkZqlativbt26NevXqSBkdERPQqDE5uRb3CgIiITIvS25IGV6bm5ua4c+dOofX379+Hubm5JEEREZFxmQnSLeWRwcmtuKd1abVaWFhYlDogIiKi0ipxW3Lp0qUAnpWyX3/9NWxsbHTb8vLykJCQwGtuREQmorxWXFIpcXJbsmQJgGeV28qVK/VakBYWFvD09MTKlSulj5CIiCSn9GtuJU5uaWlpAIAOHTpg27ZtcHR0NFpQREREpWHwbMlDhw4ZIw4iIipDSm9LGjyh5O2338ann35aaP2CBQvw7rvvShIUEREZlyBIt5RHBie3hIQEdOvWrdD6rl27IiEhQZKgiIiISsPgtuTjx4+LnPJfsWJFZGZmShIUEREZl9JfeWNw5ebn54eNGzcWWv/999/Dx8dHkqCIiMi4zCRcyiODK7eIiAj07t0bV65cwb/+9S8AwMGDB7F+/Xps2bJF8gCJiIgMZXByCwoKQlxcHObNm4ctW7bAysoKjRo1Qnx8PF95Q0RkIhTelXy197l1794d3bt3BwBkZmZiw4YNmDhxIs6cOYO8vDxJAyQiIunxmlsxEhISEBoaCjc3N3z22Wf417/+hePHj0sZGxER0SsxqHJLT0/HmjVr8M033yAzMxN9+vSBVqtFXFwcJ5MQEZkQhRduJa/cgoKC4O3tjfPnzyM6Oho3b97EsmXLjBkbEREZidJfeVPiym3Pnj0YO3YsRo4cCS8vL2PGREREVColrtyOHj2Kv/76C82aNUPLli2xfPly3Lt3z5ixERGRkZgJgmRLeVTi5PbGG2/gq6++wq1btzBixAh8//33cHNzQ35+Pg4cOIC//vrLmHESEZGE+GzJ51hbW2Po0KE4evQoUlJSMGHCBMyfPx/VqlVDjx49jBEjERGRQUr15BRvb28sWLAA//vf/7BhwwapYiIiIiPjhJISMDc3R3BwMIKDg6UYjoiIjExAOc1KEimvz7wkIiJ6ZZJUbkREZFrKaztRKkxuREQqpPTkxrYkEREpDis3IiIVEsrrDWoSYXIjIlIhtiWJiIhMDCs3IiIVUnhXksmNiEiNyusDj6XCtiQRESkOkxsRkQrJ+WzJP//8E//+97/h5OQEKysr+Pn54fTp05KeH9uSREQqJFdX8uHDh/D390eHDh2wZ88eVK1aFZcvX4ajo6Okx2FyIyKiMvPpp5+iZs2aWL16tW5drVq1JD8O25JERCpkBkGyRavVIjMzU2/RarVFHnfnzp1o3rw53n33XVSrVg1NmjTBV199ZYTzIyIi1ZHyTdxRUVGwt7fXW6Kiooo87tWrV7FixQp4eXlh3759GDlyJMaOHYvY2Fhpz08URVHSEcuBnL/ljoDUwrHFaLlDIJXIPrtc0vG+OHZNsrHea+ZaqFLTaDTQaDSF9rWwsEDz5s1x7Ngx3bqxY8fi1KlTSExMlCwmXnMjIlIhKR+/VVwiK4qrqyt8fHz01tWvXx9bt26VLiAwuRERqZJcN3H7+/vj4sWLeusuXboEDw8PSY/Da25ERFRmxo8fj+PHj2PevHlITU3F+vXrsWrVKowaNUrS4zC5ERGpkJQTSgzRokULbN++HRs2bECDBg0we/ZsREdHY8CAAZKeH9uSREQqJOezJQMDAxEYGGjUY7ByIyIixWHlRkSkQgp/KQCTGxGRGim9baf08yMiIhVi5UZEpEKCwvuSTG5ERCqk7NTGtiQRESkQKzciIhWS8z63ssDkRkSkQspObWxLEhGRArFyIyJSIYV3JZnciIjUSOm3ArAtSUREisPKjYhIhZRe2TC5ERGpENuSREREJoaVGxGRCim7bmNyIyJSJbYliYiITAwrNyIiFVJ6ZcPkRkSkQmxLEhERmRhWbkREKqTsuo3JjYhIlRTelWRbkoiIlIeVGxGRCpkpvDHJ5EZEpEJsSxIREZmYcpvcbt++jVmzZskdBhGRIgkS/lceldvklp6ejpkzZ8odBhGRIgmCdEt5JNs1t/Pnz79w+8WLF8soEiIiUhrZklvjxo0hCAJEUSy0rWC90h8PQ0QkF86WNJLKlStjwYIF6NixY5HbL1y4gKCgoDKOiohIHZReO8iW3Jo1a4abN2/Cw8OjyO2PHj0qsqojIiJ6GdmS2wcffICsrKxit7u7u2P16tVlGBERkXqwcjOSXr16vXC7o6MjQkNDyygaIiJ1Ka9T+KVSbm8FICIielV8/BYRkQqZKbtwY3IjIlIjtiWJiIhMDJMbEZEKlYfHb82fPx+CIGDcuHGSnVcB2ZPb3r17cfToUd3PMTExaNy4Mfr374+HDx/KGBkRkXLJ/eDkU6dO4csvv0TDhg0lPrNnZE9ukyZNQmZmJgAgJSUFEyZMQLdu3ZCWloawsDCZoyMiIqk9fvwYAwYMwFdffQVHR0ejHEP25JaWlgYfHx8AwNatWxEYGIh58+YhJiYGe/bskTk6IiJlMhOkW7RaLTIzM/UWrVZb7LFHjRqF7t27IyAgwHjnZ7SRS8jCwgJPnjwBAPz444/o1KkTgGfPniyo6IiISFpStiWjoqJgb2+vt0RFRRV53O+//x5JSUnFbpeK7LcCtGnTBmFhYfD398fJkyexceNGAMClS5dQo0YNmaNTh+/Xr0Ps6m9w795dvOZdD1M/joCfkfrgpB7+Tetg/KAANPVxh2tVe/QZvwr/Pfz/r7paNfPfGNjjDb3P7P/5V/Qc/UVZh0qlFB4eXugykkajKbTfH3/8gY8++ggHDhyApaWlUWOSPbktX74cH374IbZs2YIVK1agevXqAIA9e/agS5cuMkenfHv3/IBFC6IwbcZM+Pk1wrq1sRg54j3s2LUXTk5OcodHJszaSoOUS3/i2x2J2Lj4/SL32ffzBYyY8Z3uZ23u32UVnupJ+WxJjUZTZDJ73pkzZ3Dnzh00bdpUty4vLw8JCQlYvnw5tFotzM3NJYlJ9uTm7u6OXbt2FVq/ZMkSGaJRn7Wxq9H7nT4I7vU2AGDajJlISDiMuG1b8d7wov9CIiqJ/T//iv0///rCfXJz/8bt+3+VUUT0T3Lcwt2xY0ekpKTorRsyZAjq1auHKVOmSJbYgHKQ3JKSklCxYkX4+fkBAHbs2IHVq1fDx8cHkZGRsLCwkDlC5Xqam4vffr2A94aP0K0zMzPDG2+0xvlzZ2WMjNTizeZeuH4wCo8yn+DwqUuYGbMLDzKKf1sImTZbW1s0aNBAb521tTWcnJwKrS8t2SeUjBgxApcuXQIAXL16FSEhIahUqRI2b96MyZMnv/Tzhs7Sof/38NFD5OXlFWo/Ojk54d69ezJFRWpx4NhvGBaxFt1GLMO0z3fgzWZ1sWP5SJgp/aGH5YSZIEi2lEeyJ7dLly6hcePGAIDNmzejbdu2WL9+PdasWYOtW7e+9PNFzdJZ+KlxZ+EQUelt3ncGu4+k4ELqTfz38Hn0HrsSzRt4om1zL7lDUwVBwqU0Dh8+jOjo6FKOUpjsbUlRFJGfnw/g2a0AgYGBAICaNWuWqHooapaOaP7yC5sEODo4wtzcHPfv39dbf//+fVSpUkWmqEitrv15H3cf/oU6Navi8MlLcodDJk72yq158+aYM2cO1q5diyNHjqB79+4Ant3c7ezs/NLPazQa2NnZ6S0lmbVDQEULC9T38cWJ44m6dfn5+ThxIhENGzWRMTJSo+rVHOBkb430e7y/tUyUl9LNSGSv3KKjozFgwADExcXhk08+Qd26dQEAW7ZsQevWrWWOTvkGhg5BxMdT4OvbAA38GuK7tbHIzs5GcK/ecodGJs7aygJ1albV/exZ3QkNX6uOh5lP8CAjC5+M6Ia4g8lIv5eJ2jWrYO5Hwbjyxz0cOPabjFGrh9JfeSOIoijKHURRcnJyYG5ujooVKxr+Wd4qY5AN677T3cTtXa8+pnw8DQ0bNpI7LJPg2GK03CGUW28288L+rz8qtH7tzuMYO28jNi1+H43q1YCDrRVu3c3Aj4m/Y9YXu3DnAW8NKEr22eWSjnfiSoZkY7WsYy/ZWFIpt8mtNJjcqKwwuVFZkTq5nbwqXXJ7vXb5S26ytyXz8vKwZMkSbNq0CTdu3EBubq7e9gcPHsgUGRGRcim7KVkOJpTMnDkTixcvRt++fZGRkYGwsDD07t0bZmZmiIyMlDs8IiIyQbInt3Xr1uGrr77ChAkTUKFCBfTr1w9ff/01pk+fjuPHj8sdHhGRMil8tqTsyS09PV336C0bGxtkZDzrAwcGBmL37t1yhkZEpFhyv4nb2GRPbjVq1MCtW7cAAHXq1MH+/fsBPHsFOe9XIyKiVyF7cuvVqxcOHjwIABgzZgwiIiLg5eWFQYMGYejQoTJHR0SkTIIg3VIeyT5bcv78+bo/9+3bF+7u7khMTISXlxeCgoJkjIyIiEyV7Mntea1atUKrVq3kDoOISNHKacElGVmS286dO0u8b48ePYwYCRGRSik8u8mS3IKDg0u0nyAIyMvLM24wRESkOLIkt4JX3BARkTzK6xR+qZS7a25ERGR85XWWo1RkuxUgPj4ePj4+yMws/O6mjIwM+Pr6IiEhQYbIiIjI1MmW3KKjozF8+HDY2dkV2mZvb48RI0ZgyZIlMkRGRKR8Cn/6lnzJ7dy5c+jSpUux2zt16oQzZ86UYURERCqi8OwmW3K7ffv2C19EWqFCBdy9e7cMIyIiIqWQLblVr14dv/zyS7Hbz58/D1dX1zKMiIhIPfjgZCPp1q0bIiIikJOTU2hbdnY2ZsyYgcDAQBkiIyJSPqU/W1IQRVGU48C3b99G06ZNYW5ujtGjR8Pb2xsA8PvvvyMmJgZ5eXlISkqCs7OzwWPn/C11tERFc2wxWu4QSCWyzy6XdLyU/z2WbCy/GjaSjSUV2e5zc3Z2xrFjxzBy5EiEh4ejIMcKgoDOnTsjJibmlRIbERG9XDktuCQj603cHh4e+OGHH/Dw4UOkpqZCFEV4eXnB0dFRzrCIiJRP4dmtXDyhxNHRES1atJA7DCIiUohykdyIiKhslddZjlJhciMiUqHyOstRKrLdCkBERGQsrNyIiFRI4YUbkxsRkSopPLuxLUlERIrDyo2ISIU4W5KIiBSHsyWJiIhMDCs3IiIVUnjhxuRGRKRKCs9ubEsSEZHisHIjIlIhpc+WZOVGRKRCcr2JOyoqCi1atICtrS2qVauG4OBgXLx4UfLzY3IjIqIyc+TIEYwaNQrHjx/HgQMH8PTpU3Tq1AlZWVmSHodtSSIiFZKrKbl37169n9esWYNq1arhzJkzaNu2rWTHYXIjIlIjCbObVquFVqvVW6fRaKDRaF762YyMDABA5cqVpQsIbEsSEVEpRUVFwd7eXm+Jiop66efy8/Mxbtw4+Pv7o0GDBpLGJIiiKEo6YjmQ87fcEZBaOLYYLXcIpBLZZ5dLOt7VuzmSjVXdTnilym3kyJHYs2cPjh49iho1akgWD8C2JBGRKkn5bMmStiD/afTo0di1axcSEhIkT2wAkxsREZUhURQxZswYbN++HYcPH0atWrWMchwmNyIiFZJrtuSoUaOwfv167NixA7a2tkhPTwcA2Nvbw8rKSrLjcEIJEZEaCRIuBlixYgUyMjLQvn17uLq66paNGzdKcVY6rNyIiKjMlNUcRiY3IiIVUvqzJZnciIhUiG/iJiIiMjGs3IiIVEjhhRuTGxGRGrEtSUREZGJYuRERqZKySzcmNyIiFWJbkoiIyMSwciMiUiGFF25MbkREasS2JBERkYlh5UZEpEJ8tiQRESmPsnMb25JERKQ8rNyIiFRI4YUbkxsRkRpxtiQREZGJYeVGRKRCnC1JRETKo+zcxrYkEREpDys3IiIVUnjhxuRGRKRGnC1JRERkYli5ERGpEGdLEhGR4rAtSUREZGKY3IiISHHYliQiUiG2JYmIiEwMKzciIhXibEkiIlIctiWJiIhMDCs3IiIVUnjhxuRGRKRKCs9ubEsSEZHisHIjIlIhzpYkIiLF4WxJIiIiE8PKjYhIhRReuDG5ERGpksKzG9uSRERU5mJiYuDp6QlLS0u0bNkSJ0+elHR8JjciIhUSJPzPUBs3bkRYWBhmzJiBpKQkNGrUCJ07d8adO3ckOz8mNyIiFRIE6RZDLV68GMOHD8eQIUPg4+ODlStXolKlSvjPf/4j2fkxuRERUalotVpkZmbqLVqttsh9c3NzcebMGQQEBOjWmZmZISAgAImJiZLFpMgJJZaKPCvj0mq1iIqKQnh4ODQajdzhmIzss8vlDsHk8HetfJDy78nIOVGYOXOm3roZM2YgMjKy0L737t1DXl4enJ2d9dY7Ozvj999/lywmQRRFUbLRyGRlZmbC3t4eGRkZsLOzkzscUjD+rimPVqstVKlpNJoi//Fy8+ZNVK9eHceOHUOrVq106ydPnowjR47gxIkTksTEGoeIiEqluERWlCpVqsDc3By3b9/WW3/79m24uLhIFhOvuRERUZmxsLBAs2bNcPDgQd26/Px8HDx4UK+SKy1WbkREVKbCwsIQGhqK5s2b4/XXX0d0dDSysrIwZMgQyY7B5EYAnrUVZsyYwQv8ZHT8XaO+ffvi7t27mD59OtLT09G4cWPs3bu30CST0uCEEiIiUhxecyMiIsVhciMiIsVhciMiIsVhclMgQRAQFxcndxikAvxdo/KKyc3EpKenY8yYMahduzY0Gg1q1qyJoKAgvXtG5CSKIqZPnw5XV1dYWVkhICAAly9fljssegXl/Xdt27Zt6NSpE5ycnCAIApKTk+UOicoRJjcTcu3aNTRr1gzx8fFYuHAhUlJSsHfvXnTo0AGjRo2SOzwAwIIFC7B06VKsXLkSJ06cgLW1NTp37oycnBy5QyMDmMLvWlZWFtq0aYNPP/1U7lCoPBLJZHTt2lWsXr26+Pjx40LbHj58qPszAHH79u26nydPnix6eXmJVlZWYq1atcRp06aJubm5uu3Jycli+/btRRsbG9HW1lZs2rSpeOrUKVEURfHatWtiYGCg6ODgIFaqVEn08fERd+/eXWR8+fn5oouLi7hw4ULdukePHokajUbcsGFDKc+eylJ5/137p7S0NBGAePbs2Vc+X1Ie3sRtIh48eIC9e/di7ty5sLa2LrTdwcGh2M/a2tpizZo1cHNzQ0pKCoYPHw5bW1tMnjwZADBgwAA0adIEK1asgLm5OZKTk1GxYkUAwKhRo5Cbm4uEhARYW1vj119/hY2NTZHHSUtLQ3p6ut6rLOzt7dGyZUskJiYiJCSkFN8AlRVT+F0jehkmNxORmpoKURRRr149gz87bdo03Z89PT0xceJEfP/997q/cG7cuIFJkybpxvby8tLtf+PGDbz99tvw8/MDANSuXbvY46SnpwNAka+yKNhG5Z8p/K4RvQyvuZkIsRQPktm4cSP8/f3h4uICGxsbTJs2DTdu3NBtDwsLw7BhwxAQEID58+fjypUrum1jx47FnDlz4O/vjxkzZuD8+fOlOg8q//i7RkrA5GYivLy8IAiCwS/zS0xMxIABA9CtWzfs2rULZ8+exSeffILc3FzdPpGRkbhw4QK6d++O+Ph4+Pj4YPv27QCAYcOG4erVqxg4cCBSUlLQvHlzLFu2rMhjFbyuwtivsiDjMoXfNaKXkveSHxmiS5cuBl/kX7RokVi7dm29fd977z3R3t6+2OOEhISIQUFBRW6bOnWq6OfnV+S2ggklixYt0q3LyMjghBITVN5/1/6JE0qoKKzcTEhMTAzy8vLw+uuvY+vWrbh8+TJ+++03LF26tNj3IHl5eeHGjRv4/vvvceXKFSxdulT3L2UAyM7OxujRo3H48GFcv34dP//8M06dOoX69esDAMaNG4d9+/YhLS0NSUlJOHTokG7b8wRBwLhx4zBnzhzs3LkTKSkpGDRoENzc3BAcHCz590HGU95/14BnE1+Sk5Px66+/AgAuXryI5ORkXt+lZ+TOrmSYmzdviqNGjRI9PDxECwsLsXr16mKPHj3EQ4cO6fbBc9OzJ02aJDo5OYk2NjZi3759xSVLluj+Na3VasWQkBCxZs2aooWFhejm5iaOHj1azM7OFkVRFEePHi3WqVNH1Gg0YtWqVcWBAweK9+7dKza+/Px8MSIiQnR2dhY1Go3YsWNH8eLFi8b4KsjIyvvv2urVq0UAhZYZM2YY4dsgU8NX3hARkeKwLUlERIrD5EZERIrD5EZERIrD5EZERIrD5EZERIrD5EZERIrD5EZERIrD5EZERIrD5EZUQoMHD9Z7jFj79u0xbty4Mo/j8OHDEAQBjx49KvNjE5kKJjcyeYMHD4YgCBAEARYWFqhbty5mzZqFv//+26jH3bZtG2bPnl2ifZmQiMoWX1ZKitClSxesXr0aWq0WP/zwA0aNGoWKFSsiPDxcb7/c3FxYWFhIcszKlStLMg4RSY+VGymCRqOBi4sLPDw8MHLkSAQEBGDnzp26VuLcuXPh5uYGb29vAMAff/yBPn36wMHBAZUrV0bPnj1x7do13Xh5eXkICwuDg4MDnJycMHny5EIv8Xy+LanVajFlyhTUrFkTGo0GdevWxTfffINr166hQ4cOAABHR0cIgoDBgwcDAPLz8xEVFYVatWrBysoKjRo1wpYtW/SO88MPP+C1116DlZUVOnTooBcnERWNyY0UycrKSveSzIMHD+LixYs4cOAAdu3ahadPn6Jz586wtbXFTz/9hJ9//hk2Njbo0qWL7jOfffYZ1qxZg//85z84evQoHjx4oPf6lqIMGjQIGzZswNKlS/Hbb7/hyy+/hI2NDWrWrImtW7cCePZallu3buHzzz8HAERFReHbb7/FypUrceHCBYwfPx7//ve/ceTIEQDPknDv3r0RFBSE5ORkDBs2DFOnTjXW10akHDK/lYCo1EJDQ8WePXuKovjslTsHDhwQNRqNOHHiRDE0NFR0dnYWtVqtbv+1a9eK3t7eYn5+vm6dVqsVraysxH379omiKIqurq7iggULdNufPn0q1qhRQ3ccURTFdu3aiR999JEoiqJ48eJFEYB44MCBImM8dOiQCEDvRZ85OTlipUqVxGPHjunt+95774n9+vUTRVEUw8PDRR8fH73tU6ZMKTQWEenjNTdShF27dsHGxgZPnz5Ffn4++vfvj8jISIwaNQp+fn5619nOnTuH1NRU2Nra6o2Rk5ODK1euICMjA7du3ULLli112ypUqIDmzZsXak0WSE5Ohrm5Odq1a1fimFNTU/HkyRO89dZbeutzc3PRpEkTAMBvv/2mFweAYl8WSkT/j8mNFKFDhw5YsWIFLCws4ObmhgoV/v9X29raWm/fx48fo1mzZli3bl2hcapWrfpKx7eysjL4M48fPwYA7N69G9WrV9fbptFoXikOInqGyY0UwdraGnXr1i3Rvk2bNsXGjRtRrVo12NnZFbmPq6srTpw4gbZt2wIA/v77b5w5cwZNmzYtcn8/Pz/k5+fjyJEjCAgIKLS9oHLMy8vTrfPx8YFGo8GNGzeKrfjq16+PnTt36q07fvz4y0+SSOU4oYRUZ8CAAahSpQp69uyJn376CWlpaTh8+DDGjh2L//3vfwCAjz76CPPnz0dcXBx+//13fPjhhy+8R83T0xOhoaEYOnQo4uLidGNu2rQJAODh4QFBELBr1y7cvXsXjx8/hq2tLSZOnIjx48cjNjYWV65cQVJSEpYtW4bY2FgAwAcffIDLly9j0qRJuHjxItavX481a9YY+ysiMnlMbqQ6lSpVQkJCAtzd3dG7d2/Ur18f7733HnJycnSV3IQJEzBw4ECEhoaiVatWsLW1Ra9evV447ooVK/DOO+/gww8/RL169TB8+HBkZWUBAKpXr46ZM2di6tSpcHZ2xujRowEAs2fPRkREBKKiolC/fn106dIFu3fvRq1atQAA7u7u2Lp1K+Li4tCoUSOsXLkS8+bNM+K3Q6QMgljcFXIiIiITxcqNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgUh8mNiIgU5/8AK4dyRN1qhrwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQFjSqJHMKea",
        "outputId": "5f7328ba-148e-401a-cfa8-a0521aa9d0d9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with class weights:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlO6QYhwMLB4",
        "outputId": "a6e7cd95-6ecc-45df-aa6f-049426191c2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with class weights: 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.read_csv('titanic.csv')\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data['Age'] = imputer.fit_transform(data[['Age']])\n",
        "data = data.dropna(subset=['Embarked'])\n",
        "data = data.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "X = data.drop(columns=['Survived'])\n",
        "y = data['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "il7qbhYvMN1o",
        "outputId": "6a012943-cf8f-42ab-a3d9-d106502e6c0c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-86df68e1fc59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train and evaluate without scaling\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply standardization (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate with scaling\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjwu2yl6MOzd",
        "outputId": "785c674c-d85c-4b87-eed9-3b36519a0058"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0\n",
            "Accuracy with scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "\n",
        "roc_auc = roc_auc_score(y_test_bin, y_pred_prob, multi_class='ovr')\n",
        "\n",
        "print(\"ROC-AUC score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg5oa0kdMP4J",
        "outputId": "681eb31a-14fc-4004-ff97-3f159ee290ed"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4eLxpYMRAf",
        "outputId": "dc6d82b0-82c4-434f-a9a2-63889edae154"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "coefficients = model.coef_\n",
        "\n",
        "important_features = pd.DataFrame(coefficients, columns=feature_names)\n",
        "print(\"Feature importance (based on coefficients):\")\n",
        "print(important_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeSY4rb7MSO9",
        "outputId": "c1d061d5-e05f-4e45-df61-49ce28c79c92"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance (based on coefficients):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0          -0.393402          0.962586          -2.375108         -0.998746\n",
            "1           0.508404         -0.254865          -0.213014         -0.775755\n",
            "2          -0.115002         -0.707721           2.588121          1.774501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIvwrE8MTEM",
        "outputId": "de498dff-127e-4f7f-f7df-c1297e4613d6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "0_pHOeqHMUBp",
        "outputId": "c387c3ba-a241-4a86-f1b3-d393b3491571"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou1bkgWONpf8",
        "outputId": "6813680d-2941-4ade-c4fe-afda7ce7aa30"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver 'liblinear': 1.0\n",
            "Accuracy with solver 'saga': 1.0\n",
            "Accuracy with solver 'lbfgs': 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL_3VdMbNrKI",
        "outputId": "c42700af-c4ac-4c32-a64f-896cac6a7725"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy on raw data:\", accuracy_raw)\n",
        "print(\"Accuracy on standardized data:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guW1jZ6PNsUE",
        "outputId": "bfa84230-e2cf-4a8d-e9e4-715d0eaaa5a0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 1.0\n",
            "Accuracy on standardized data: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(\"Optimal C (regularization strength):\", best_C)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy with optimal C:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpOe_hqnNtQ2",
        "outputId": "a4bac3b6-ddb7-4a41-b5bb-cb13e0a853c4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C (regularization strength): 1\n",
            "Accuracy with optimal C: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the loaded model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEjTE9n1Nuf3",
        "outputId": "c7a3dcf7-2ed5-4594-d912-93f5ff981962"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the loaded model: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcOiwPTINvlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}